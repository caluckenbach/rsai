use std::{env, marker::PhantomData};

use tracing::{debug, instrument};

use crate::{
    provider::{Provider, gemini, openai, openrouter},
    responses::HttpClientConfig,
};

use super::{
    error::LlmError,
    traits::LlmProvider,
    types::{
        ConversationMessage, GenerationConfig, Message, StructuredRequest, ToolChoice, ToolConfig,
        ToolRegistry,
    },
};

mod private {
    pub struct ProviderSet;
    pub struct ApiKeySet;
    pub struct Configuring;
    pub struct MessagesSet;
    pub struct ToolsSet;

    /// Marker trait for states that can call complete()
    pub trait Completable {}
    impl Completable for MessagesSet {}
    impl Completable for ToolsSet {}
}

/// Builder fields that are shared across all states.
/// Generic over context type `Ctx` for tool execution.
struct BuilderFields<Ctx = ()> {
    // Core configuration
    provider: Option<Provider>,
    api_key: Option<String>,
    model: Option<String>,
    http_client_config: Option<HttpClientConfig>,

    // Request content
    messages: Option<Vec<Message>>,

    // Tool configuration
    tool_choice: Option<ToolChoice>,
    parallel_tool_calls: Option<bool>,
    tool_registry: Option<ToolRegistry<Ctx>>,

    // Generation parameters
    max_tokens: Option<u32>,
    temperature: Option<f32>,
    top_p: Option<f32>,
}

impl BuilderFields<()> {
    fn new() -> Self {
        Self {
            provider: None,
            api_key: None,
            model: None,
            messages: None,
            tool_choice: None,
            parallel_tool_calls: None,
            tool_registry: None,
            max_tokens: None,
            temperature: None,
            top_p: None,
            http_client_config: None,
        }
    }
}

impl<Ctx> BuilderFields<Ctx> {
    /// Create a new BuilderFields with a different context type
    fn with_context_type<NewCtx>(self, tool_registry: Option<ToolRegistry<NewCtx>>) -> BuilderFields<NewCtx> {
        BuilderFields {
            provider: self.provider,
            api_key: self.api_key,
            model: self.model,
            http_client_config: self.http_client_config,
            messages: self.messages,
            tool_choice: self.tool_choice,
            parallel_tool_calls: self.parallel_tool_calls,
            tool_registry,
            max_tokens: self.max_tokens,
            temperature: self.temperature,
            top_p: self.top_p,
        }
    }

    /// Validate that all required fields are present
    fn validate(&self) -> Result<(&Vec<Message>, Provider, &str), LlmError> {
        self.api_key.as_ref().ok_or(LlmError::Builder(
            "Missing API key. Make sure to specify an API key.".into(),
        ))?;

        let messages = self
            .messages
            .as_ref()
            .filter(|messages| !messages.is_empty())
            .ok_or(LlmError::Builder(
                "Missing messages. Make sure to add at least one message.".to_string(),
            ))?;

        let provider = self.provider.ok_or(LlmError::Builder(
            "Missing provider. Make sure to specify a provider.".into(),
        ))?;

        let model = self.model.as_ref().ok_or(LlmError::Builder(
            "Missing model. Make sure to specify a model.".into(),
        ))?;

        Ok((messages, provider, model))
    }
}

/// A type-safe builder for constructing LLM requests using the builder pattern.
/// The builder enforces correct construction order through phantom types.
///
/// Type parameters:
/// - `State`: The current builder state (ProviderSet, ApiKeySet, MessagesSet, ToolsSet)
/// - `Ctx`: The context type for tool execution (defaults to `()` for no context)
pub struct LlmBuilder<State, Ctx = ()> {
    fields: BuilderFields<Ctx>,
    _state: PhantomData<State>,
}

impl<State, Ctx> LlmBuilder<State, Ctx> {
    /// Transition to a new builder state while preserving all field values
    fn transition_state<NewState>(self) -> LlmBuilder<NewState, Ctx> {
        LlmBuilder {
            fields: self.fields,
            _state: PhantomData,
        }
    }

    pub(crate) fn get_api_key(&self) -> Option<&str> {
        self.fields.api_key.as_deref()
    }

    pub(crate) fn get_http_config(&self) -> Option<&HttpClientConfig> {
        self.fields.http_client_config.as_ref()
    }
}

/// Configuration for API key source
pub enum ApiKey {
    /// Use the default environment variable for the provider
    Default,
    /// Use a custom API key string
    Custom(String),
}

impl LlmBuilder<private::ProviderSet, ()> {
    /// Set the API key for the provider.
    /// Use `ApiKey::Default` to load from environment variables or `ApiKey::Custom` for a custom key.
    pub fn api_key(mut self, api_key: ApiKey) -> Result<LlmBuilder<private::ApiKeySet, ()>, LlmError> {
        let key = match api_key {
            ApiKey::Default => {
                let provider = self.fields.provider.ok_or(LlmError::Builder(
                    "Provider must be set before API key".into(),
                ))?;

                env::var(provider.default_api_key_env_var()).map_err(|_| {
                    LlmError::Builder(format!(
                        "Missing {} environment variable",
                        provider.default_api_key_env_var()
                    ))
                })?
            }
            ApiKey::Custom(custom_key) => custom_key,
        };

        self.fields.api_key = Some(key);
        Ok(self.transition_state())
    }
}

impl LlmBuilder<private::ApiKeySet, ()> {
    /// Set the model to use for the LLM request.
    pub fn model(mut self, model_id: &str) -> LlmBuilder<private::Configuring, ()> {
        self.fields.model = Some(model_id.to_string());
        self.transition_state()
    }
}

impl LlmBuilder<private::Configuring, ()> {
    /// Set the messages for the conversation.
    pub fn messages(mut self, messages: Vec<Message>) -> LlmBuilder<private::MessagesSet, ()> {
        self.fields.messages = Some(messages);
        self.transition_state()
    }
}

impl<State: private::Completable, Ctx: Send + Sync + 'static> LlmBuilder<State, Ctx> {
    /// Set a custom timeout for the HTTP request.
    /// This is a convenience method that modifies the HttpClientConfig.
    pub fn timeout(mut self, duration: std::time::Duration) -> Self {
        let mut config = self.fields.http_client_config.unwrap_or_default();
        config.timeout = duration;
        self.fields.http_client_config = Some(config);
        self
    }

    /// Set the full HTTP client configuration (retries, backoff, etc).
    pub fn http_client_config(mut self, config: HttpClientConfig) -> Self {
        self.fields.http_client_config = Some(config);
        self
    }

    /// Set the maximum number of tokens to generate.
    pub fn max_tokens(mut self, max_tokens: u32) -> Self {
        self.fields.max_tokens = Some(max_tokens);
        self
    }

    /// Set the temperature for generation (0.0 to 2.0).
    /// Lower values make output more focused and deterministic.
    pub fn temperature(mut self, temperature: f32) -> Self {
        self.fields.temperature = Some(temperature);
        self
    }

    /// Set the top_p value for nucleus sampling (0.0 to 1.0).
    /// An alternative to temperature; use one or the other, not both.
    pub fn top_p(mut self, top_p: f32) -> Self {
        self.fields.top_p = Some(top_p);
        self
    }

    /// Execute the LLM request and return an output defined by `T`.
    ///
    /// The target type `T` must implement [`CompletionTarget`]. Structured schemas can be created
    /// with the `#[completion_schema]` macro, while plain text responses can use [`TextResponse`].
    ///
    /// # Example
    /// ```no_run
    /// # use rsai::{completion_schema, llm, Message, ChatRole, ApiKey, Provider, TextResponse};
    /// # #[tokio::main]
    /// # async fn main() -> Result<(), Box<dyn std::error::Error>> {
    /// #[completion_schema]
    /// struct Analysis {
    ///     sentiment: String,
    ///     confidence: f32,
    /// }
    ///
    /// let analysis = llm::with(Provider::OpenAI)
    ///     .api_key(ApiKey::Default)?
    ///     .model("gpt-4o-mini")
    ///     .messages(vec![Message {
    ///         role: ChatRole::User,
    ///         content: "Analyze: 'This library is amazing!'".to_string(),
    ///     }])
    ///     .complete::<Analysis>()
    ///     .await?;
    ///
    /// let text = llm::with(Provider::OpenAI)
    ///     .api_key(ApiKey::Default)?
    ///     .model("gpt-4o-mini")
    ///     .messages(vec![Message {
    ///         role: ChatRole::User,
    ///         content: "Say hello".to_string(),
    ///     }])
    ///     .complete::<TextResponse>()
    ///     .await?;
    /// # Ok(())
    /// # }
    /// ```
    #[instrument(
        name = "generate_completion",
        skip(self),
        fields(
            model = ?self.fields.model,
            provider = ?self.fields.provider,
            max_tokens = ?self.fields.max_tokens,
        ),
        err
    )]
    pub async fn complete<T>(self) -> Result<T::Output, LlmError>
    where
        T: super::traits::CompletionTarget + Send,
    {
        debug!("Starting generation request");
        let (messages, provider, model) = self.fields.validate()?;
        let model_string = model.to_string();
        let messages = messages.to_vec();
        let format = T::format()?;

        if !T::supports_tools() && self.fields.tool_registry.is_some() {
            return Err(LlmError::Builder(
                "Tools are only supported with structured completion targets".to_string(),
            ));
        }

        // Deferred error handling for tool registry errors in case of a poisoned lock.
        let tool_schemas = if T::supports_tools() {
            self.fields
                .tool_registry
                .as_ref()
                .map(|registry| registry.get_schemas())
                .transpose()?
                .map(|tools| tools.into_boxed_slice())
        } else {
            None
        };

        match provider {
            Provider::OpenAI => {
                let conversation_messages: Vec<ConversationMessage> = messages
                    .into_iter()
                    .map(ConversationMessage::Chat)
                    .collect();

                let req = StructuredRequest {
                    model: model_string,
                    messages: conversation_messages,
                    tool_config: tool_schemas.map(|tools| ToolConfig {
                        tools: Some(tools),
                        tool_choice: self.fields.tool_choice.clone(),
                        parallel_tool_calls: self.fields.parallel_tool_calls,
                    }),
                    generation_config: Some(GenerationConfig {
                        max_tokens: self.fields.max_tokens,
                        temperature: self.fields.temperature,
                        top_p: self.fields.top_p,
                    }),
                };
                let client = openai::create_openai_client_from_builder(&self)?;
                client
                    .generate_completion::<T, Ctx>(
                        req,
                        format.clone(),
                        self.fields.tool_registry.as_ref(),
                    )
                    .await
            }
            Provider::OpenRouter => {
                let conversation_messages: Vec<ConversationMessage> = messages
                    .into_iter()
                    .map(ConversationMessage::Chat)
                    .collect();

                let req = StructuredRequest {
                    model: model_string,
                    messages: conversation_messages,
                    tool_config: tool_schemas.map(|tools| ToolConfig {
                        tools: Some(tools),
                        tool_choice: self.fields.tool_choice.clone(),
                        parallel_tool_calls: self.fields.parallel_tool_calls,
                    }),
                    generation_config: Some(GenerationConfig {
                        max_tokens: self.fields.max_tokens,
                        temperature: self.fields.temperature,
                        top_p: self.fields.top_p,
                    }),
                };
                let client = openrouter::create_openrouter_client_from_builder(&self)?;
                client
                    .generate_completion::<T, Ctx>(req, format, self.fields.tool_registry.as_ref())
                    .await
            }
            Provider::Gemini => {
                let conversation_messages: Vec<ConversationMessage> = messages
                    .into_iter()
                    .map(ConversationMessage::Chat)
                    .collect();

                let req = StructuredRequest {
                    model: model_string,
                    messages: conversation_messages,
                    tool_config: tool_schemas.map(|tools| ToolConfig {
                        tools: Some(tools),
                        tool_choice: self.fields.tool_choice.clone(),
                        parallel_tool_calls: self.fields.parallel_tool_calls,
                    }),
                    generation_config: Some(GenerationConfig {
                        max_tokens: self.fields.max_tokens,
                        temperature: self.fields.temperature,
                        top_p: self.fields.top_p,
                    }),
                };
                let client = gemini::create_gemini_client_from_builder(&self)?;
                client
                    .generate_completion::<T, Ctx>(req, format, self.fields.tool_registry.as_ref())
                    .await
            }
        }
    }
}

impl LlmBuilder<private::MessagesSet, ()> {
    /// Set the tools for the LLM request with automatic execution support.
    /// This transitions to the ToolsSet state where tool_choice and parallel_tool_calls can be configured.
    ///
    /// By default parallel tool calling is enabled. This can be changed by calling `parallel_tool_calls` with `false`.
    pub fn tools<NewCtx: Send + Sync + 'static>(
        mut self,
        toolset: super::types::ToolSet<NewCtx>,
    ) -> LlmBuilder<private::ToolsSet, NewCtx> {
        self.fields.parallel_tool_calls = Some(true);
        let new_fields = self.fields.with_context_type(Some(toolset.registry));
        LlmBuilder {
            fields: new_fields,
            _state: PhantomData,
        }
    }
}

impl<Ctx: Send + Sync + 'static> LlmBuilder<private::ToolsSet, Ctx> {
    pub fn tool_choice(mut self, choice: ToolChoice) -> Self {
        self.fields.tool_choice = Some(choice);
        self
    }

    /// Set whether to enable parallel tool calls.
    /// When true, the model can call multiple tools simultaneously.
    pub fn parallel_tool_calls(mut self, enabled: bool) -> Self {
        self.fields.parallel_tool_calls = Some(enabled);
        self
    }
}

/// Module containing the main entry point for building LLM requests
pub mod llm {
    use super::*;

    /// Create a new LLM builder with the specified provider.
    ///
    /// # Example
    /// ```no_run
    /// # use rsai::{llm, ApiKey, Provider};
    /// # #[tokio::main]
    /// # async fn main() -> Result<(), Box<dyn std::error::Error>> {
    /// let builder = llm::with(Provider::OpenAI)
    ///     .api_key(ApiKey::Default)?
    ///     .model("gpt-4o-mini");
    /// # Ok(())
    /// # }
    /// ```
    pub fn with(provider: Provider) -> LlmBuilder<private::ProviderSet, ()> {
        let mut fields = BuilderFields::new();
        fields.provider = Some(provider);

        LlmBuilder {
            fields,
            _state: PhantomData,
        }
    }
}
